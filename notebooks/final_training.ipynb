{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtqBu9p17D2j"
      },
      "source": [
        "# Training the final model\n",
        "\n",
        "This notebook is supplementary material to the project here, which aims to re-implement the Hubble.2d6 tool to predict the function of CYP2D6 star alleles.\n",
        "\n",
        "Within this notebook, the final model in the sequence outlined in the paper is trained using transfer learning. Weights from the first step are loaded into the new model. The fully-connected layers of the first model are replaced with new, randomly initialised layers and trained on 31 star alleles and their respective suballeles.\n",
        "\n",
        "Please keep in mind that this implementation is incomplete due to the lack of resources to compute robust annotation embeddings of all variants for all star alleles.\n",
        "\n",
        "Additionally, in the paper, this model is supposed to instead have its weights - along with weights of one of the fully-connected layers - loaded in from the second model in the sequence. However, due to data availability, I was unable to reproduce the implementation of the second model and instead chose to transfer just the convolution layers' weights to the final model from the first model that was trained on simulated data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8a-EgmE8R4T"
      },
      "source": [
        "## Getting ready\n",
        "\n",
        "**Acknowledgements**: Pre-computed annotation embeddings used are form the original Hubble.2d6 repo: https://github.com/gregmcinnes/Hubble2D6/tree/master/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMaqb2OooOXz"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcVVyrq6uMF_"
      },
      "source": [
        "np.random.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaZ5UF2MoTfj",
        "outputId": "d6374633-1a5c-4a10-972e-0a3eef2c82d0"
      },
      "source": [
        "!git clone https://github.com/Locrian24/seng474-term-project.git\n",
        "!cd seng474-term-project/ && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seng474-term-project'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 95 (delta 38), reused 75 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GWlFbEPofie"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/seng474-term-project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGj8AD838tOw"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "The vcf file of the star alleles used were provided from the paper and can be found **here**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N78uh8bB819L"
      },
      "source": [
        "### Encoding method\n",
        "\n",
        "The script `build_label_csv.py` was run on this vcf file to output the label csv that will be used for training.\n",
        "\n",
        "Labels for the data correspond to the ordinal classes describing the function of each CYP2D6 star allele and their respective suballeles: \"No function\", \"Decreased function\", and \"Normal function\".\n",
        "\n",
        "The model however is required to output two scores, representing the probability of a star allele being \"No function\" and \"Normal function\" respectively, and therefore a binary scoring system is used to encode the 3 function classes into these two scores. \n",
        "\n",
        "The scoring system is as follows:\n",
        "- \"No function\" alleles are indicated with a 0 as the first score, with all other functions being denoted with a 1\n",
        "- \"Normal function\" alleles are indicated with a 1 as the second score, with all other functions being denoted with a 0.\n",
        "\n",
        "This yields the following encodings:\n",
        "- \"No function\" = `[0, 0]`\n",
        "- \"Decreased function\" = `[0, 1]`\n",
        "- \"Normal function\" = `[1, 1]`\n",
        "\n",
        "**Note**: Star alleles with uncurated function have an empty label (`[None, None]`). \n",
        "\n",
        "_Sequence encodings are done exactly the same as in step 1_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laWNZhUIo2Bl"
      },
      "source": [
        "from encode_to_seq import Encode2Seq\n",
        "\n",
        "# Global variables rn for testing\n",
        "\n",
        "ANNOTATIONS = '/content/seng474-term-project/data/gvcf2seq.annotation_embeddings.csv'\n",
        "EMBEDDINGS = '/content/seng474-term-project/data/embeddings.txt'\n",
        "REF = '/content/seng474-term-project/data/ref.seq'\n",
        "\n",
        "VCF = '/content/seng474-term-project/step3/star_samples.vcf'\n",
        "LABELS = '/content/seng474-term-project/step3/labels.csv'\n",
        "\n",
        "encoding = Encode2Seq(vcf=VCF, labels=LABELS, label_cols=[0, 1, 2], embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFolDssq-wRu"
      },
      "source": [
        "### Seperation of data sets\n",
        "\n",
        "The paper indicates that they used 31 star alleles and their suballeles as training data, and 24 alleles to compose the test data. Additionally, 10% of the each functional class (\"No function\", \"Decreased function\", and \"Normal function\") were held out for validation during training.\n",
        "\n",
        "All star alleles with \"Uncurated function\" were ignored, and held from training/testing for obvious reasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQ9jyQMp_ca"
      },
      "source": [
        "# Select only star alleles with curated function\n",
        "\n",
        "mask = np.all(np.isnan(encoding.y) == False, axis=1)\n",
        "\n",
        "sample_y = encoding.y[mask]\n",
        "sample_X = encoding.X[mask.reshape(-1, 1).any(axis=1)]\n",
        "sample_names = encoding.sample_names[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIJzGV8s8G3"
      },
      "source": [
        "# Get valid stars\n",
        "all_stars = np.array([s.split('_')[1] for s in sample_names])\n",
        "stars, idx = np.unique(all_stars, return_index=True)\n",
        "\n",
        "# Choose which stars are training and which are test: (31, 24) split\n",
        "# Should be stratified with labels\n",
        "train_idx, test_idx = train_test_split(idx, stratify=sample_y[idx], test_size=24, random_state=0)\n",
        "\n",
        "# Retrieve indices of training and test stars\n",
        "sample_mask = np.isin(all_stars, all_stars[train_idx])\n",
        "\n",
        "test_stars = np.array([s for s in sample_names[~sample_mask] if s.split('_')[-1] == 'vcf'])\n",
        "test_mask = np.isin(sample_names, test_stars)\n",
        "\n",
        "# Split the data into the two sets - INCLUDING SUBALLES ON BOTH\n",
        "# _train_X, test_X = sample_X[sample_mask], sample_X[~sample_mask]\n",
        "# _train_y, test_y = sample_y[sample_mask], sample_y[~sample_mask]\n",
        "\n",
        "_train_X, test_X = sample_X[sample_mask], sample_X[test_mask]\n",
        "_train_y, test_y = sample_y[sample_mask], sample_y[test_mask]\n",
        "\n",
        "# Split training into train + validation (10% split -> validation)\n",
        "train_X, val_X, train_y, val_y = train_test_split(_train_X, _train_y, stratify=_train_y, test_size=0.1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAUG1QZQootJ"
      },
      "source": [
        "# Uncurated star alleles\n",
        "\n",
        "uncurated_samples = encoding.sample_names[~mask]\n",
        "uncurated_stars = np.array([s for s in uncurated_samples if s.split('_')[-1] == 'vcf'])\n",
        "uncurated_star_mask = np.isin(uncurated_samples, uncurated_stars)\n",
        "\n",
        "# uncurated_X = encoding.X[uncurated_star_idx]\n",
        "# uncurated_samples = uncurated_samples[uncurated_star_idx]\n",
        "\n",
        "uncurated_samples = uncurated_samples[uncurated_star_mask]\n",
        "uncurated_X = encoding.X[(~mask).reshape(-1, 1).any(axis=1)]\n",
        "uncurated_X = uncurated_X[uncurated_star_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpx-XJedAIRj"
      },
      "source": [
        "### Prepare data for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysEE5cl2AG6O"
      },
      "source": [
        "_train_ds = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
        "train_ds = _train_ds.shuffle(32).batch(32).prefetch(buffer_size=10)\n",
        "\n",
        "_val_ds = tf.data.Dataset.from_tensor_slices((val_X, val_y))\n",
        "val_ds = _val_ds.shuffle(32).batch(32).prefetch(buffer_size=10)\n",
        "\n",
        "_test_ds = tf.data.Dataset.from_tensors((test_X, test_y))\n",
        "test_ds = _test_ds.prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcKwYCgH6WN5"
      },
      "source": [
        "## Building the final model\n",
        "\n",
        "As stated earlier, the model is loaded from the generated model from step 1, and also inherits its learned weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFFfrnMRMzDz"
      },
      "source": [
        "epochs = 20\n",
        "fine_tune_epochs = 7\n",
        "\n",
        "def build_and_fit(train_dataset, val_dataset):\n",
        "  json_file = open('/content/seng474-term-project/step_1/model.json', 'r')\n",
        "  loaded_model = json_file.read()\n",
        "  model = tf.keras.models.model_from_json(loaded_model)\n",
        "  model.load_weights('/content/seng474-term-project/step_1/weights.h5')\n",
        "  \n",
        "  # Remove fully connected layers\n",
        "  model.pop()\n",
        "  model.pop()\n",
        "  model.pop()\n",
        "  model.trainable = False\n",
        "\n",
        "  # Build final model\n",
        "  inputs = tf.keras.Input(shape=(14868, 13))\n",
        "  # model.training = False\n",
        "  x = model(inputs, training=False)\n",
        "  x = tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"dense_5\")(x)\n",
        "  x = tf.keras.layers.Dropout(rate=0.03, name=\"dropout_4\")(x)\n",
        "  x = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"dense_6\")(x)\n",
        "  outputs = tf.keras.layers.Dense(2, activation=tf.keras.activations.sigmoid, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"final_dense\")(x)\n",
        "  final_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  # Initial training\n",
        "  final_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "\n",
        "  final_model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, verbose=0)\n",
        "\n",
        "  # Fine tuning\n",
        "  model.trainable = True\n",
        "  # model.training = True\n",
        "  final_model.compile(\n",
        "      tf.keras.optimizers.Adam(1e-5),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "\n",
        "  final_model.fit(train_dataset, epochs=fine_tune_epochs, validation_data=val_dataset, verbose=0)\n",
        "  \n",
        "  return final_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPINycCNcOV4"
      },
      "source": [
        "## Training ensemble models\n",
        "\n",
        "Hubble.2d6 uses an ensemble averaging method for it's final predictions. Here we are training 7 models to use in the ensemble, report the training accuracy of each and save these models for the final tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF3D87O0a4On",
        "outputId": "a621b359-3c19-498a-e65a-0f6f5d649be6"
      },
      "source": [
        "ensemble_size = 7\n",
        "\n",
        "for i in range(ensemble_size):\n",
        "  model = build_and_fit(train_ds, val_ds)\n",
        "  model.evaluate(train_ds)\n",
        "  model.evaluate(val_ds)\n",
        "\n",
        "  model.save(f'models/ensemble_{i}.model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 81ms/step - loss: 0.1318 - binary_accuracy: 0.9353\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.5213 - binary_accuracy: 0.8077\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 0.1235 - binary_accuracy: 0.9397\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5532 - binary_accuracy: 0.8077\n",
            "4/4 [==============================] - 1s 78ms/step - loss: 0.5258 - binary_accuracy: 0.6897\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6190 - binary_accuracy: 0.6538\n",
            "4/4 [==============================] - 1s 81ms/step - loss: 0.1169 - binary_accuracy: 0.9698\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.4278 - binary_accuracy: 0.8077\n",
            "4/4 [==============================] - 1s 80ms/step - loss: 0.1252 - binary_accuracy: 0.9440\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5430 - binary_accuracy: 0.8077\n",
            "4/4 [==============================] - 0s 82ms/step - loss: 0.1356 - binary_accuracy: 0.9353\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.4769 - binary_accuracy: 0.8077\n",
            "4/4 [==============================] - 0s 78ms/step - loss: 0.1409 - binary_accuracy: 0.9353\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.4126 - binary_accuracy: 0.8077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hffB4EkIlov8"
      },
      "source": [
        "Now that we have the ensemble models saved, we will use them for the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOyYP5sfr0b2"
      },
      "source": [
        "## Saving evaluation data\n",
        "\n",
        "We are saving the test dataset for our evaluation step on the ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nJWnzprsGta"
      },
      "source": [
        "np.savetxt(\"test.samples.csv\", test_X.reshape(test_X.shape[0], 14868 * 13), delimiter=\",\")\n",
        "np.savetxt(\"test.labels.csv\", test_y, delimiter=\",\")"
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}