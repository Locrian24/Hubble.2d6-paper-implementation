{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_step_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPZ5GFVXMSGj"
      },
      "source": [
        "# !wget https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz -O simulated.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0INgaw2nM5ML"
      },
      "source": [
        "# !tar -xsf simulated.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99C_S-l9M9VN"
      },
      "source": [
        "# !ls -1 ./simulated_cyp2d6_diplotypes/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqSzUz8jUHFr"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UCBCEKQ5fi",
        "outputId": "596cbc2f-da6f-443c-f547-ce4e53375296"
      },
      "source": [
        "!git clone https://github.com/Locrian24/seng474-term-project.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seng474-term-project'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 67 (delta 25), reused 52 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC0TgiJoOABC",
        "outputId": "de983f22-808f-4483-9cce-9b0e3b0172a6"
      },
      "source": [
        "!cd seng474-term-project/ && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AftpF2WzQC3G",
        "outputId": "a03dd120-441d-4701-85e3-8a58c2e3c713"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xtYrkXQIVU"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/seng474-term-project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM7QlhWBQtWX"
      },
      "source": [
        "from encode_to_seq import Encode2Seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaKzkIi2QwYx"
      },
      "source": [
        "# Global variables rn for testing\n",
        "\n",
        "ANNOTATIONS = '/content/seng474-term-project/data/gvcf2seq.annotation_embeddings.csv'\n",
        "EMBEDDINGS = '/content/seng474-term-project/data/embeddings.txt'\n",
        "REF = '/content/seng474-term-project/data/ref.seq'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTigmIJrR2l5"
      },
      "source": [
        "# encoding = FirstStep2Seq(vcf=VCF, labels=LABEL, annotation_file=ANNOTATIONS, embedding_file=EMBEDDINGS, ref_seq=REF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxtY_pqiTx9r"
      },
      "source": [
        "# encoding.X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p34MYJU0UMzo",
        "outputId": "0f7670cf-608e-47f3-89b2-51cbbe6e3ac9"
      },
      "source": [
        "print(tf.__version__)\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/classification_iris_data_with_keras.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WonJWOrB6A10"
      },
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_batch_files(training_count, test_count):\n",
        "\n",
        "  file_root = tf.keras.utils.get_file(\n",
        "      'simulated_cyp2d6_diplotypes',\n",
        "      'https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz',\n",
        "      untar=True\n",
        "  )\n",
        "  file_root = pathlib.Path(file_root)\n",
        "  filenames = []\n",
        "  for f in file_root.glob(\"*\"):\n",
        "    filenames.append(f)\n",
        "\n",
        "  _filenames = np.array([f.name.split('.')[0] for f in filenames])\n",
        "  batch_names = np.unique(_filenames)\n",
        "  filenames = np.array([str(f.absolute()) for f in filenames])\n",
        "  training_batches, test_batches = [], []\n",
        "\n",
        "  for i, b in enumerate(batch_names):\n",
        "    if i >= test_count + training_count:\n",
        "      break\n",
        "      \n",
        "    if i < training_count:\n",
        "      training_batches.append(filenames[_filenames == b])\n",
        "    else:\n",
        "      test_batches.append(filenames[_filenames == b])\n",
        "\n",
        "  return training_batches, test_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzqQKOIJt9Ir"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def hot_encode_float(y):\n",
        "  classes = []\n",
        "  values = np.unique(y)\n",
        "  for i in range(len(values)):\n",
        "    classes.append(str(i))\n",
        "  encoded_classes = to_categorical(classes)\n",
        "  conversion_dict = dict(zip(values, range(5)))\n",
        "  encoded_y = np.array([encoded_classes[conversion_dict[i]] for i in y])\n",
        "\n",
        "  return encoded_y\n",
        "\n",
        "def generate_data(batches):\n",
        "  for filenames in batches:\n",
        "    vcf = 0 if 'vcf' == filenames[0].decode('utf-8').split('.')[-1] else 1\n",
        "    labels = 1 - vcf\n",
        "    encoding = Encode2Seq(vcf=filenames[vcf].decode('utf-8'), labels=filenames[labels].decode('utf-8'), embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "    y = hot_encode_float(encoding.y.flatten())\n",
        "    for i in range(encoding.X.shape[0]):\n",
        "      yield encoding.X[i], y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBI2753VWw_"
      },
      "source": [
        "# First train on one file to make sure things work\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fhXvtIyWfqL"
      },
      "source": [
        "def get_model():\n",
        "  return keras.Sequential([\n",
        "    keras.layers.Conv1D(70, kernel_size=19, strides=5,input_shape=(14868, 13), batch_input_shape=(None, 14868, 13), activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_1\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_1\"),\n",
        "    keras.layers.ReLU(name=\"relu_1\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=3, strides=3, name=\"maxpooling_1\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=11, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_2\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_2\"),\n",
        "    keras.layers.ReLU(name=\"relu_2\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_2\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=7, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_3\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_3\"),\n",
        "    keras.layers.ReLU(name=\"relu_3\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_3\"),\n",
        "    keras.layers.Flatten(name=\"flatten_3\"),\n",
        "    keras.layers.Dense(32, activation=keras.activations.relu, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_4\"),\n",
        "    keras.layers.Dropout(rate=0.03, name=\"dropout_4\"),\n",
        "    keras.layers.Dense(5, activation='softmax', kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_5\"),\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yxPKHd_Sq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e007c8-979d-4803-b336-236b9eefd288"
      },
      "source": [
        "training_batches, test_batches = get_batch_files(100, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz\n",
            "22437888/22436828 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWuNAaro594x"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(generate_data, args=[training_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B70ek76SfDL"
      },
      "source": [
        "batch_size = 100\n",
        "steps_per_epoch = 50000 // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEGQL6tCJFq"
      },
      "source": [
        "batched = train_dataset.shuffle(500).repeat(count=5).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbD5g68aQcNT"
      },
      "source": [
        "# with tf.device('/device:GPU:0'):\n",
        "model = get_model()\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam,\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])\n",
        "model_json = model.to_json()\n",
        "# with open(\"model.json\", \"w\") as json:\n",
        "#   json.write(model_json)\n",
        "  \n",
        "model.load_weights('/content/seng474-term-project/step_1/weights.h5')\n",
        "  # model.fit(batched, epochs=2, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTgGnR2hsk3"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(generate_data, args=[test_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW5he7Ifh6tY"
      },
      "source": [
        "sample = tf.data.experimental.sample_from_datasets([test_dataset])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1Pfbi8az0G"
      },
      "source": [
        "# model.save_weights('weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHDtifaLtOPC"
      },
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf7PGMNzmNKr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "fbce841e-a184-4eda-88d7-546339887724"
      },
      "source": [
        "for filenames in test_batches:\n",
        "  vcf = 0 if 'vcf' == filenames[0].split('.')[-1] else 1\n",
        "  labels = 1 - vcf\n",
        "  encoding = Encode2Seq(vcf=filenames[vcf], labels=filenames[labels], embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "  y = hot_encode_float(encoding.y.flatten())\n",
        "  X, y = shuffle(encoding.X, y)\n",
        "  print(np.argmax(model.predict(X), axis=1))\n",
        "  print(model.evaluate(X, y))\n",
        "\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5ef18dd9bf98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mvcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'vcf'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvcf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFirstStep2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvcf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDINGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mANNOTATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhot_encode_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FirstStep2Seq' is not defined"
          ]
        }
      ]
    }
  ]
}