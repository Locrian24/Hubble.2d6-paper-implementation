{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_step_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gxXTNKJRGLH"
      },
      "source": [
        "# Training first model on simulated CYP2D6 diplotypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvrzYGFbqYct"
      },
      "source": [
        "This notebook is supplementary material to the project here, which aims to re-implement the Hubble.2d6 tool to predict the function of CYP2D6 star alleles.\n",
        "\n",
        "Within this notebook, the 1st model is trained using simulated CYP2D6 diplotype data provided by the original paper. This model's weights will be transfered to the final model to be fine-tuned for prediction of CYP2D6 phenotypes.\n",
        "\n",
        "Please keep in mind that the encoding process of the provided data is incomplete due to my situation and technical restrictions around programs I have available at my disposal. More information on the actual implementation can be read in the final report. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIbbU-QjtR6e"
      },
      "source": [
        "## Getting ready\n",
        "\n",
        "**Acknowledgements**: Pre-computed annotation embeddings used are from the original Hubble.2d6 repo: https://github.com/gregmcinnes/Hubble2D6/tree/master/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqSzUz8jUHFr"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UCBCEKQ5fi",
        "outputId": "48d6fd50-deac-48d1-e49c-b257fdcdeb74"
      },
      "source": [
        "!git clone https://github.com/Locrian24/seng474-term-project.git\n",
        "!cd seng474-term-project/ && git pull"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seng474-term-project'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 87 (delta 33), reused 69 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xtYrkXQIVU"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/seng474-term-project')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2AJCWhMu6LH"
      },
      "source": [
        "## GPU Runtime\n",
        "\n",
        "Before running this notebook, make sure your hardware accelerator is a GPU by selecting **GPU** from the settings: **Runtime -> Change runtime type -> Hardware accelerator -> GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AftpF2WzQC3G"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '':\n",
        "  raise SystemExit(\"Dataset generator is built to run on the GPU runtime. Please switch to GPU by selecting GPU from Runtime -> Change runtime type\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EI2O1CvV2n"
      },
      "source": [
        "## Retrieving the data\n",
        "\n",
        "These functions are responsible for loading the simulated data onto disk and getting the batch files ready for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WonJWOrB6A10"
      },
      "source": [
        "import pathlib\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def get_batch_files(training_count, test_count):\n",
        "  \"\"\"\n",
        "  Pull simulated data from zenodo and split the batch files into training and testing sets\n",
        "  \"\"\"\n",
        "\n",
        "  file_root = tf.keras.utils.get_file(\n",
        "      'simulated_cyp2d6_diplotypes',\n",
        "      'https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz',\n",
        "      untar=True\n",
        "  )\n",
        "  file_root = pathlib.Path(file_root)\n",
        "  filenames = []\n",
        "  for f in file_root.glob(\"*\"):\n",
        "    filenames.append(f)\n",
        "\n",
        "  _filenames = np.array([f.name.split('.')[0] for f in filenames])\n",
        "  batch_names = np.unique(_filenames)\n",
        "  filenames = np.array([str(f.absolute()) for f in filenames])\n",
        "  training_batches, test_batches = [], []\n",
        "\n",
        "  for i, b in enumerate(batch_names):\n",
        "    if i >= test_count + training_count:\n",
        "      break\n",
        "      \n",
        "    if i < training_count:\n",
        "      training_batches.append(filenames[_filenames == b])\n",
        "    else:\n",
        "      test_batches.append(filenames[_filenames == b])\n",
        "\n",
        "  return training_batches, test_batches\n",
        "\n",
        "def hot_encode_float(y):\n",
        "  \"\"\"\n",
        "  This is ultimately a classification problem and so the labels must be encoded appropriately\n",
        "  One-hot encodes the activity scores within the label vector\n",
        "  \"\"\"\n",
        "  \n",
        "  classes = []\n",
        "  values = np.unique(y)\n",
        "  for i in range(len(values)):\n",
        "    classes.append(str(i))\n",
        "  encoded_classes = to_categorical(classes)\n",
        "  conversion_dict = dict(zip(values, range(5)))\n",
        "  encoded_y = np.array([encoded_classes[conversion_dict[i]] for i in y])\n",
        "\n",
        "  return encoded_y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-OodDsv2s9"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "The batch files available from the original paper are in vcf format and must be converted to a one-hot encoded + annotation format to be passed into the model. \n",
        "\n",
        "Encode2Seq compares variants within the vcf files to the reference seq, and updates the sequences of each diplotype before then using the pre-computed annotation embeddings to one-hot encode, annotate, and match the samples with their labels.\n",
        "\n",
        "**Acknowledgements**: Encode2Seq is forked from the method used within the Hubble.2d6 tool. It was expanded to handle diplotype encodings since the base method is for single haplotypes only.\n",
        "\n",
        "***Important***: Since Encode2Seq only has access to pre-computed embedding data, some variants within the simulated vcf files do not have corresponding embeddings and so have empty annotation vectors. In total, 317 of the 1406 variants do not have corresponding pre-computing annotation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iQk07SSj44l"
      },
      "source": [
        "from encode_to_seq import Encode2Seq\n",
        "\n",
        "ANNOTATIONS = '/content/seng474-term-project/data/gvcf2seq.annotation_embeddings.csv'\n",
        "EMBEDDINGS = '/content/seng474-term-project/data/embeddings.txt'\n",
        "REF = '/content/seng474-term-project/data/ref.seq'\n",
        "\n",
        "def generate_data(batches):\n",
        "  \"\"\"\n",
        "  Generator that encodes and yields samples one at a time for implementation purposes.\n",
        "  Manually storing these encodings would be beyond my abilities/resources, so I chose to encode them within a generator and pass the encoded data directly to the model\n",
        "  \"\"\"\n",
        "\n",
        "  for filenames in batches:\n",
        "    vcf = 0 if 'vcf' == filenames[0].decode('utf-8').split('.')[-1] else 1\n",
        "    labels = 1 - vcf\n",
        "    encoding = Encode2Seq(vcf=filenames[vcf].decode('utf-8'), labels=filenames[labels].decode('utf-8'), embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "    y = hot_encode_float(encoding.y.flatten())\n",
        "    for i in range(encoding.X.shape[0]):\n",
        "      yield encoding.X[i], y[i]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obmWqmO0a-a"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fhXvtIyWfqL"
      },
      "source": [
        "# Convolution layers based on final model from paper:\n",
        "# https://github.com/gregmcinnes/Hubble2D6/blob/master/data/models/hubble2d6_0.json\n",
        "\n",
        "def get_model():\n",
        "  return tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(70, kernel_size=19, strides=5,input_shape=(14868, 13), batch_input_shape=(None, 14868, 13), activation=tf.keras.activations.linear, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_1\"),\n",
        "    tf.keras.layers.BatchNormalization(name=\"batch_1\"),\n",
        "    tf.keras.layers.ReLU(name=\"relu_1\"),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=3, strides=3, name=\"maxpooling_1\"),\n",
        "    tf.keras.layers.Conv1D(46, kernel_size=11, strides=5, activation=tf.keras.activations.linear, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_2\"),\n",
        "    tf.keras.layers.BatchNormalization(name=\"batch_2\"),\n",
        "    tf.keras.layers.ReLU(name=\"relu_2\"),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_2\"),\n",
        "    tf.keras.layers.Conv1D(46, kernel_size=7, strides=5, activation=tf.keras.activations.linear, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_3\"),\n",
        "    tf.keras.layers.BatchNormalization(name=\"batch_3\"),\n",
        "    tf.keras.layers.ReLU(name=\"relu_3\"),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_3\"),\n",
        "    tf.keras.layers.Flatten(name=\"flatten_3\"),\n",
        "    tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_4\"),\n",
        "    tf.keras.layers.Dropout(rate=0.03, name=\"dropout_4\"),\n",
        "    tf.keras.layers.Dense(5, activation='softmax', kernel_initializer=tf.keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_5\"),\n",
        "  ])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbD5g68aQcNT"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model = get_model()\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=adam,\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhXxFMmd0fVx"
      },
      "source": [
        "### Preparing the batch files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B70ek76SfDL"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 5\n",
        "steps_per_epoch = 50000 // batch_size"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yxPKHd_Sq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f89d2c-38a7-4a66-f3d9-19d09b2f42cb"
      },
      "source": [
        "# Provided training data contains 250,000 samples (500 samples per batch)\n",
        "# Selecting 50,000 samples for training, and 10,000 for testing as per the paper specifications\n",
        "\n",
        "training_batches, test_batches = get_batch_files(100, 20)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz\n",
            "22437888/22436828 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWuNAaro594x"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(generate_data, args=[training_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))\n",
        "test_dataset = tf.data.Dataset.from_generator(generate_data, args=[test_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(500).repeat(count=5).batch(batch_size)\n",
        "test_dataset = test_dataset.batch(500)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQFH-llg0o3l"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "Training the model takes around 25-30 minutes running on a GPU. \n",
        "\n",
        "I've commented the `fit` call out and loaded in the weights for convienence but feel free to train the initial model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9FrfpbpR6JR",
        "outputId": "039433cb-e212-4660-a9f7-4b6cc43fd71e"
      },
      "source": [
        "# model.load_weights('/content/seng474-term-project/step_1/weights.h5')\n",
        "model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 337s 594ms/step - loss: 1.5210 - accuracy: 0.3013\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - 307s 611ms/step - loss: 0.9555 - accuracy: 0.5968\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - 298s 592ms/step - loss: 0.5202 - accuracy: 0.7984\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - 296s 588ms/step - loss: 0.3193 - accuracy: 0.8755\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - 291s 578ms/step - loss: 0.2134 - accuracy: 0.9199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f18204d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xViFjGIag728"
      },
      "source": [
        "# model.save_weights('weights.h5')\n",
        "\n",
        "# json_model = model.to_json()\n",
        "# with open(\"model.json\", \"w\") as json:\n",
        "#   json.write(model_json)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQWpiH4-SKYg"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoKIqiRUh_vF",
        "outputId": "64f0e8ed-3cc5-4055-8e8a-304e989d9f8c"
      },
      "source": [
        "model.evaluate(test_dataset, steps=20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 55s 3s/step - loss: 0.5240 - accuracy: 0.8302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5240103006362915, 0.8302000164985657]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jciKMSSntIE"
      },
      "source": [
        "As you can see, the model attains around 80% accuracy on the testing set.\n",
        "\n",
        "Note, the original implementation of Hubble.2d6 attains an accuracy of 100% on its testing set. This discrepancy could be a result of many factors, including training time/procedure as well as more robust training set or embeddings."
      ]
    }
  ]
}