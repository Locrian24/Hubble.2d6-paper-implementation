{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_step_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPZ5GFVXMSGj"
      },
      "source": [
        "# !wget https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz -O simulated.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0INgaw2nM5ML"
      },
      "source": [
        "# !tar -xsf simulated.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99C_S-l9M9VN"
      },
      "source": [
        "# !ls -1 ./simulated_cyp2d6_diplotypes/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqSzUz8jUHFr"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UCBCEKQ5fi",
        "outputId": "d4b6debd-55ce-4f6e-c94d-a88c7b29abae"
      },
      "source": [
        "!git clone https://github.com/Locrian24/seng474-term-project.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seng474-term-project'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 43 (delta 13), reused 34 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC0TgiJoOABC",
        "outputId": "5a6d89ab-507c-4d1a-ffc5-274fe3fe4e8f"
      },
      "source": [
        "!cd seng474-term-project/ && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AftpF2WzQC3G",
        "outputId": "4870c5fb-bc08-47bb-daad-599bf0b6f122"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_smbhETUeLJ"
      },
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xtYrkXQIVU"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/seng474-term-project/step_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM7QlhWBQtWX"
      },
      "source": [
        "from encode_to_seq import Encode2Seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaKzkIi2QwYx"
      },
      "source": [
        "# Global variables rn for testing\n",
        "\n",
        "ANNOTATIONS = '/content/seng474-term-project/data/gvcf2seq.annotation_embeddings.csv'\n",
        "EMBEDDINGS = '/content/seng474-term-project/data/embeddings.txt'\n",
        "REF = '/content/seng474-term-project/data/ref.seq'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTigmIJrR2l5"
      },
      "source": [
        "# encoding = FirstStep2Seq(vcf=VCF, labels=LABEL, annotation_file=ANNOTATIONS, embedding_file=EMBEDDINGS, ref_seq=REF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxtY_pqiTx9r"
      },
      "source": [
        "# encoding.X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p34MYJU0UMzo",
        "outputId": "76550eaa-dbc4-4591-a76e-5a08a286b545"
      },
      "source": [
        "print(tf.__version__)\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/classification_iris_data_with_keras.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WonJWOrB6A10"
      },
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_batch_files(training_count, test_count):\n",
        "\n",
        "  file_root = tf.keras.utils.get_file(\n",
        "      'simulated_cyp2d6_diplotypes',\n",
        "      'https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz',\n",
        "      untar=True\n",
        "  )\n",
        "  file_root = pathlib.Path(file_root)\n",
        "  filenames = []\n",
        "  for f in file_root.glob(\"*\"):\n",
        "    filenames.append(f)\n",
        "\n",
        "  _filenames = np.array([f.name.split('.')[0] for f in filenames])\n",
        "  batch_names = np.unique(_filenames)\n",
        "  filenames = np.array([str(f.absolute()) for f in filenames])\n",
        "  training_batches, test_batches = [], []\n",
        "\n",
        "  for i, b in enumerate(batch_names):\n",
        "    if i >= test_count + training_count:\n",
        "      break\n",
        "      \n",
        "    if i < training_count:\n",
        "      training_batches.append(filenames[_filenames == b])\n",
        "    else:\n",
        "      test_batches.append(filenames[_filenames == b])\n",
        "\n",
        "  return training_batches, test_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzqQKOIJt9Ir"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def hot_encode_float(y):\n",
        "  classes = []\n",
        "  values = np.unique(y)\n",
        "  for i in range(len(values)):\n",
        "    classes.append(str(i))\n",
        "  encoded_classes = to_categorical(classes)\n",
        "  conversion_dict = dict(zip(values, range(5)))\n",
        "  encoded_y = np.array([encoded_classes[conversion_dict[i]] for i in y])\n",
        "\n",
        "  return encoded_y\n",
        "\n",
        "def generate_data(batches):\n",
        "  for filenames in batches:\n",
        "    vcf = 0 if 'vcf' == filenames[0].decode('utf-8').split('.')[-1] else 1\n",
        "    labels = 1 - vcf\n",
        "    encoding = Encode2Seq(vcf=filenames[vcf].decode('utf-8'), labels=filenames[labels].decode('utf-8'), embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "    y = hot_encode_float(encoding.y.flatten())\n",
        "    for i in range(encoding.X.shape[0]):\n",
        "      yield encoding.X[i], y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBI2753VWw_"
      },
      "source": [
        "# First train on one file to make sure things work\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fhXvtIyWfqL"
      },
      "source": [
        "def get_model():\n",
        "  return keras.Sequential([\n",
        "    keras.layers.Conv1D(70, kernel_size=19, strides=5,input_shape=(14868, 13), batch_input_shape=(None, 14868, 13), activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_1\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_1\"),\n",
        "    keras.layers.ReLU(name=\"relu_1\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=3, strides=3, name=\"maxpooling_1\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=11, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_2\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_2\"),\n",
        "    keras.layers.ReLU(name=\"relu_2\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_2\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=7, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_3\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_3\"),\n",
        "    keras.layers.ReLU(name=\"relu_3\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_3\"),\n",
        "    keras.layers.Flatten(name=\"flatten_3\"),\n",
        "    keras.layers.Dense(32, activation=keras.activations.relu, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_4\"),\n",
        "    keras.layers.Dropout(rate=0.03, name=\"dropout_4\"),\n",
        "    keras.layers.Dense(5, activation='softmax', kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_5\"),\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yxPKHd_Sq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07507610-c030-4ba9-86d3-eb49c2db8609"
      },
      "source": [
        "training_batches, test_batches = get_batch_files(100, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz\n",
            "22437888/22436828 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWuNAaro594x"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(generate_data, args=[training_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B70ek76SfDL"
      },
      "source": [
        "batch_size = 100\n",
        "steps_per_epoch = 50000 // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEGQL6tCJFq"
      },
      "source": [
        "batched = train_dataset.shuffle(500).repeat(count=5).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbD5g68aQcNT"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model = get_model()\n",
        "  adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=adam,\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "                metrics=['accuracy'])\n",
        "  model.load_weights('/content/seng474-term-project/step_1/weights.h5')\n",
        "  # model.fit(batched, epochs=2, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTgGnR2hsk3"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(generate_data, args=[test_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW5he7Ifh6tY"
      },
      "source": [
        "sample = tf.data.experimental.sample_from_datasets([test_dataset])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1Pfbi8az0G"
      },
      "source": [
        "# model.save_weights('weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHDtifaLtOPC"
      },
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf7PGMNzmNKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e5ec4f-9445-4606-e620-fd21de5bddb2"
      },
      "source": [
        "for filenames in test_batches:\n",
        "  vcf = 0 if 'vcf' == filenames[0].split('.')[-1] else 1\n",
        "  labels = 1 - vcf\n",
        "  encoding = FirstStep2Seq(vcf=filenames[vcf], labels=filenames[labels], embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "  y = hot_encode_float(encoding.y.flatten())\n",
        "  X, y = shuffle(encoding.X, y)\n",
        "  print(np.argmax(model.predict(X), axis=1))\n",
        "  print(model.evaluate(X, y))\n",
        "\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 3 2 0 2 2 2 4 4 2 4 2 1 0 3 3 3 3 4 1 3 0 3 2 4 3 1 1 3 4 2 3 2 1 4 2\n",
            " 3 3 3 2 2 4 2 3 2 3 2 2 2 2 0 1 4 0 0 2 4 1 4 3 1 2 3 1 2 2 1 4 2 3 2 3 1\n",
            " 4 3 2 2 4 3 4 3 2 4 3 1 0 2 3 3 4 2 3 2 4 2 4 3 4 4 2 2 4 1 1 2 0 4 4 0 4\n",
            " 2 2 0 2 1 1 0 3 3 4 2 3 2 4 1 2 4 4 1 3 1 3 2 4 4 4 3 3 2 0 3 4 4 2 3 2 0\n",
            " 3 1 3 1 2 1 0 1 4 2 0 4 3 3 3 1 3 2 4 3 3 2 3 0 1 3 0 2 2 0 3 3 0 4 4 1 4\n",
            " 2 3 1 4 4 0 3 3 2 2 4 2 2 1 1 1 3 4 4 1 1 3 2 3 2 3 1 4 3 4 3 0 4 3 2 3 3\n",
            " 0 2 2 2 4 3 3 1 1 4 2 2 2 3 1 3 2 4 2 3 1 3 0 3 1 4 4 2 0 1 3 0 2 1 0 4 2\n",
            " 3 0 1 2 4 2 2 2 4 2 3 1 2 1 2 2 4 4 4 1 2 2 4 2 1 4 2 4 4 3 2 1 3 3 3 3 2\n",
            " 2 1 2 3 4 4 3 1 2 1 4 2 0 3 2 3 4 2 3 4 3 3 0 2 2 4 3 1 1 3 4 3 1 3 3 3 3\n",
            " 1 4 0 0 4 2 4 3 0 3 1 3 3 1 2 2 2 4 4 4 3 1 3 0 1 3 4 4 3 4 2 3 4 1 2 1 2\n",
            " 2 3 1 2 4 1 3 1 0 2 0 4 1 1 3 2 2 4 2 1 1 1 3 0 2 3 1 3 2 0 3 4 2 3 2 2 1\n",
            " 3 2 0 3 3 2 3 2 3 3 2 4 4 4 2 2 3 3 1 1 4 4 1 1 1 2 1 0 0 2 1 4 2 0 4 1 4\n",
            " 3 2 4 0 4 0 2 3 3 3 2 1 2 2 2 0 3 2 2 3 2 2 3 0 4 2 3 2 2 1 2 2 3 4 4 2 1\n",
            " 2 2 1 3 4 1 1 1 3 4 2 4 1 4 3 1 4 3 2]\n",
            "16/16 [==============================] - 1s 76ms/step - loss: 0.6115 - accuracy: 0.7800\n",
            "[0.6114824414253235, 0.7799999713897705]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}