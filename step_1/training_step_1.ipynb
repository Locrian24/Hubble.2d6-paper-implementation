{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_step_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPZ5GFVXMSGj"
      },
      "source": [
        "# !wget https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz -O simulated.tgz"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0INgaw2nM5ML"
      },
      "source": [
        "# !tar -xsf simulated.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99C_S-l9M9VN"
      },
      "source": [
        "# !ls -1 ./simulated_cyp2d6_diplotypes/ | wc -l"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqSzUz8jUHFr"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UCBCEKQ5fi",
        "outputId": "aae4c7a3-767f-4131-ade4-82b9cac4e6db"
      },
      "source": [
        "!git clone https://github.com/Locrian24/seng474-term-project.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'seng474-term-project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC0TgiJoOABC",
        "outputId": "e56d8252-1def-4e59-df01-3802db4dbc70"
      },
      "source": [
        "!cd seng474-term-project/ && git pull"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AftpF2WzQC3G",
        "outputId": "eb94d834-60be-4313-e152-0a57d465915c"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_smbhETUeLJ"
      },
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xtYrkXQIVU"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/seng474-term-project/step_1')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM7QlhWBQtWX"
      },
      "source": [
        "from encode_first_step import FirstStep2Seq"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaKzkIi2QwYx"
      },
      "source": [
        "# Global variables rn for testing\n",
        "\n",
        "ANNOTATIONS = '/content/seng474-term-project/step_1/data/gvcf2seq.annotation_embeddings.csv'\n",
        "EMBEDDINGS = '/content/seng474-term-project/step_1/data/embeddings.txt'\n",
        "REF = '/content/seng474-term-project/step_1/data/ref.seq'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTigmIJrR2l5"
      },
      "source": [
        "# encoding = FirstStep2Seq(vcf=VCF, labels=LABEL, annotation_file=ANNOTATIONS, embedding_file=EMBEDDINGS, ref_seq=REF)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxtY_pqiTx9r"
      },
      "source": [
        "# encoding.X.shape"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p34MYJU0UMzo",
        "outputId": "76ed1404-9d41-4d0e-d69c-ee456eaf7cf3"
      },
      "source": [
        "print(tf.__version__)\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/classification_iris_data_with_keras.ipynb')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WonJWOrB6A10"
      },
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_batch_files(training_count, test_count):\n",
        "\n",
        "  file_root = tf.keras.utils.get_file(\n",
        "      'simulated_cyp2d6_diplotypes',\n",
        "      'https://zenodo.org/record/3951095/files/simulated_cyp2d6_diplotypes.tar.gz',\n",
        "      untar=True\n",
        "  )\n",
        "  file_root = pathlib.Path(file_root)\n",
        "  filenames = []\n",
        "  for f in file_root.glob(\"*\"):\n",
        "    filenames.append(f)\n",
        "\n",
        "  _filenames = np.array([f.name.split('.')[0] for f in filenames])\n",
        "  batch_names = np.unique(_filenames)\n",
        "  filenames = np.array([str(f.absolute()) for f in filenames])\n",
        "  training_batches, test_batches = [], []\n",
        "\n",
        "  for i, b in enumerate(batch_names):\n",
        "    if i >= test_count + training_count:\n",
        "      break\n",
        "      \n",
        "    if i < training_count:\n",
        "      training_batches.append(filenames[_filenames == b])\n",
        "    else:\n",
        "      test_batches.append(filenames[_filenames == b])\n",
        "\n",
        "  return training_batches, test_batches"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzqQKOIJt9Ir"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def hot_encode_float(y):\n",
        "  classes = []\n",
        "  values = np.unique(y)\n",
        "  for i in range(len(values)):\n",
        "    classes.append(str(i))\n",
        "  encoded_classes = to_categorical(classes)\n",
        "  conversion_dict = dict(zip(values, range(5)))\n",
        "  encoded_y = np.array([encoded_classes[conversion_dict[i]] for i in y])\n",
        "\n",
        "  return encoded_y\n",
        "\n",
        "def generate_data(batches):\n",
        "  for filenames in batches:\n",
        "    vcf = 0 if 'vcf' == filenames[0].decode('utf-8').split('.')[-1] else 1\n",
        "    labels = 1 - vcf\n",
        "    encoding = FirstStep2Seq(vcf=filenames[vcf].decode('utf-8'), labels=filenames[labels].decode('utf-8'), embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "    y = hot_encode_float(encoding.y.flatten())\n",
        "    for i in range(encoding.X.shape[0]):\n",
        "      yield encoding.X[i], y[i]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBI2753VWw_"
      },
      "source": [
        "# First train on one file to make sure things work\n",
        "from tensorflow import keras"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fhXvtIyWfqL"
      },
      "source": [
        "def get_model():\n",
        "  return keras.Sequential([\n",
        "    keras.layers.Conv1D(70, kernel_size=19, strides=5,input_shape=(14868, 13), batch_input_shape=(None, 14868, 13), activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_1\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_1\"),\n",
        "    keras.layers.ReLU(name=\"relu_1\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=3, strides=3, name=\"maxpooling_1\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=11, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_2\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_2\"),\n",
        "    keras.layers.ReLU(name=\"relu_2\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_2\"),\n",
        "    keras.layers.Conv1D(46, kernel_size=7, strides=5, activation=keras.activations.linear, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name = \"conv1d_3\"),\n",
        "    keras.layers.BatchNormalization(name=\"batch_3\"),\n",
        "    keras.layers.ReLU(name=\"relu_3\"),\n",
        "    keras.layers.MaxPooling1D(pool_size=4, strides=4, name=\"maxpooling_3\"),\n",
        "    keras.layers.Flatten(name=\"flatten_3\"),\n",
        "    keras.layers.Dense(32, activation=keras.activations.relu, kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_4\"),\n",
        "    keras.layers.Dropout(rate=0.03, name=\"dropout_4\"),\n",
        "    keras.layers.Dense(5, activation='softmax', kernel_initializer=keras.initializers.VarianceScaling(mode='fan_avg', distribution='uniform'), name=\"dense_5\"),\n",
        "  ])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yxPKHd_Sq3"
      },
      "source": [
        "training_batches, test_batches = get_batch_files(100, 20)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWuNAaro594x"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(generate_data, args=[training_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B70ek76SfDL"
      },
      "source": [
        "batch_size = 100\n",
        "steps_per_epoch = 50000 // batch_size"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEGQL6tCJFq"
      },
      "source": [
        "batched = train_dataset.shuffle(500).repeat(count=2).batch(batch_size)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbD5g68aQcNT"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model = get_model()\n",
        "  adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=adam,\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "                metrics=['accuracy'])\n",
        "  model.load_weights('weights.h5')\n",
        "  # model.fit(batched, epochs=2, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTgGnR2hsk3"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(generate_data, args=[test_batches], output_types=(tf.float32, tf.float32), output_shapes=((14868, 13), (5,)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW5he7Ifh6tY"
      },
      "source": [
        "sample = tf.data.experimental.sample_from_datasets([test_dataset])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1Pfbi8az0G"
      },
      "source": [
        "# model.save_weights('weights.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf7PGMNzmNKr",
        "outputId": "3c9ccf70-05d4-4f07-8001-c4c4cb975048"
      },
      "source": [
        "for filenames in test_batches:\n",
        "  vcf = 0 if 'vcf' == filenames[0].split('.')[-1] else 1\n",
        "  labels = 1 - vcf\n",
        "  encoding = FirstStep2Seq(vcf=filenames[vcf], labels=filenames[labels], embedding_file=EMBEDDINGS, annotation_file=ANNOTATIONS, ref_seq=REF)\n",
        "  y = hot_encode_float(encoding.y.flatten())\n",
        "  print(model.evaluate(encoding.X, y))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6115 - accuracy: 0.7800\n",
            "[0.6114830374717712, 0.7799999713897705]\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6545 - accuracy: 0.7740\n",
            "[0.6544732451438904, 0.7739999890327454]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5879 - accuracy: 0.7840\n",
            "[0.5878723859786987, 0.7839999794960022]\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.6648 - accuracy: 0.7700\n",
            "[0.6647749543190002, 0.7699999809265137]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.6741 - accuracy: 0.7720\n",
            "[0.674068808555603, 0.7720000147819519]\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.6235 - accuracy: 0.7700\n",
            "[0.623478353023529, 0.7699999809265137]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.6597 - accuracy: 0.7780\n",
            "[0.6597424149513245, 0.777999997138977]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.5749 - accuracy: 0.8180\n",
            "[0.5749475955963135, 0.8180000185966492]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.5926 - accuracy: 0.7940\n",
            "[0.5925719141960144, 0.7940000295639038]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.6062 - accuracy: 0.7680\n",
            "[0.6061679720878601, 0.7680000066757202]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.6131 - accuracy: 0.7900\n",
            "[0.6130757927894592, 0.7900000214576721]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.6394 - accuracy: 0.7900\n",
            "[0.6393831968307495, 0.7900000214576721]\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6295 - accuracy: 0.7920\n",
            "[0.6294798851013184, 0.7919999957084656]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.5499 - accuracy: 0.8260\n",
            "[0.5498527884483337, 0.8259999752044678]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5583 - accuracy: 0.7900\n",
            "[0.5583205819129944, 0.7900000214576721]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.6432 - accuracy: 0.7940\n",
            "[0.6431841850280762, 0.7940000295639038]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.6015 - accuracy: 0.8080\n",
            "[0.6015474200248718, 0.8080000281333923]\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.6427 - accuracy: 0.7760\n",
            "[0.6426888704299927, 0.7760000228881836]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5430 - accuracy: 0.8120\n",
            "[0.5429567694664001, 0.8119999766349792]\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5918 - accuracy: 0.8000\n",
            "[0.5917735695838928, 0.800000011920929]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}